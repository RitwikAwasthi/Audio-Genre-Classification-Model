{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-12T00:16:41.324175Z",
     "start_time": "2025-04-12T00:16:34.561354Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import librosa  # or torchaudio for loading audio\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import ast"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T00:16:42.293279Z",
     "start_time": "2025-04-12T00:16:41.325183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load pre-trained processor (tokenizer) and model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wav2vec.to(device)\n",
    "wav2vec.eval()  # evaluation mode (no training in typical usage)"
   ],
   "id": "eca39740f856a1ab",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Model(\n",
       "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Wav2Vec2GroupNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): Wav2Vec2FeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Wav2Vec2Encoder(\n",
       "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "      (conv): ParametrizedConv1d(\n",
       "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _WeightNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (padding): Wav2Vec2SamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2SdpaAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T00:16:42.296882Z",
     "start_time": "2025-04-12T00:16:42.293782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import librosa\n",
    "\n",
    "def extract_wav2vec_features(audio_path, sr=16000, model=wav2vec, proc=processor, device=device):\n",
    "    # 1. Load raw audio\n",
    "    waveform, original_sr = librosa.load(audio_path, sr=sr, mono=True, duration=20)\n",
    "\n",
    "    # 2. Wav2Vec2 expects a batch of waveforms in float32\n",
    "    inputs = proc(waveform, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "    # => inputs is a dict with 'input_values' (batch_size, seq_len)\n",
    "\n",
    "    # 3. Move to device (GPU/CPU)\n",
    "    input_values = inputs[\"input_values\"].to(device)  # shape: (1, seq_len)\n",
    "\n",
    "    # 4. Forward pass through Wav2Vec2\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "        # outputs.last_hidden_state: shape (batch_size, time_steps, hidden_size)\n",
    "        # e.g., for facebook/wav2vec2-base-960h => hidden_size=768\n",
    "\n",
    "    # Typically, you might use:\n",
    "    # - outputs.last_hidden_state (per-frame embeddings, shape: (1, T, 768))\n",
    "    # - outputs.hidden_states (if you want all layer outputs)\n",
    "    # Let's use the last hidden state\n",
    "    hidden_states = outputs.last_hidden_state.squeeze(0).cpu().numpy()  # shape: (time_steps, 768)\n",
    "    return hidden_states"
   ],
   "id": "7bd2431ae0d2016c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T00:16:42.310797Z",
     "start_time": "2025-04-12T00:16:42.297572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AudioTransformerClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim, model_dim, num_heads, num_layers, num_classes, max_seq_len=1000):\n",
    "        \"\"\"\n",
    "        A simple Transformer-based classifier:\n",
    "          - Each time step = 1 segment\n",
    "          - Each segment has feature_dim features\n",
    "          - Map segments to embeddings of size model_dim\n",
    "          - Learn positional embeddings up to max_seq_len\n",
    "          - TransformerEncoder of `num_layers` layers\n",
    "          - Classification head (pooling + linear)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.model_dim = model_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # 1. Convert segment features -> model_dim\n",
    "        self.input_proj = nn.Linear(feature_dim, model_dim)\n",
    "\n",
    "        # 2. Positional embeddings\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, model_dim)\n",
    "\n",
    "        # 3. Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # 4. Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(model_dim, model_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(model_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, seq_len, feature_dim)\n",
    "        Returns: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        print(x.shape)\n",
    "        \n",
    "        # Project input features to model_dim\n",
    "        x = self.input_proj(x)  # (batch_size, seq_len, model_dim)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        # We assume seq_len <= max_seq_len\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)  # shape (1, seq_len)\n",
    "        pos_emb = self.pos_embedding(positions)  # shape (1, seq_len, model_dim)\n",
    "        x = x + pos_emb  # broadcast add\n",
    "\n",
    "        # Transformer encoding\n",
    "        x = self.transformer_encoder(x)  # (batch_size, seq_len, model_dim)\n",
    "\n",
    "        # Pooling: take the mean (or the last token, or [CLS]-like approach)\n",
    "        x = x.mean(dim=1)  # shape (batch_size, model_dim)\n",
    "\n",
    "        # Classification\n",
    "        logits = self.classifier(x)  # (batch_size, num_classes)\n",
    "        return logits\n",
    "    \n",
    "def get_all_wav_files(directory):\n",
    "    wav_files = []\n",
    "    for dirpath, _, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.wav'):\n",
    "                wav_files.append(dirpath + '/' + filename)\n",
    "    return wav_files\n",
    "\n",
    "\n",
    "class GTZANSegmentsDataset(Dataset):\n",
    "    def __init__(self, files, labels):\n",
    "        self.files = files\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.files[idx]\n",
    "\n",
    "        # Convert the string to an actual Python object (list or list of lists)\n",
    "        filename = row.split('/')[-1]\n",
    "        print(filename)\n",
    "        \n",
    "        hidden_states = extract_wav2vec_features(row, model=wav2vec, proc=processor)\n",
    "\n",
    "        # Convert to a NumPy array or directly to torch.Tensor\n",
    "        X_np = np.array(hidden_states, dtype=np.float32)\n",
    "        segments_tensor = torch.tensor(X_np, dtype=torch.float)\n",
    "\n",
    "        label = labelMap[filename.split(\".\")[0]]\n",
    "        label_tensor = torch.tensor(label)\n",
    "\n",
    "        return {\n",
    "            \"segments\": segments_tensor,\n",
    "            \"label\": label_tensor\n",
    "        }\n",
    "\n",
    "labelMap = {\n",
    "    \"blues\": 0,\n",
    "    \"classical\": 1,\n",
    "    \"country\": 2,\n",
    "    \"disco\": 3,\n",
    "    \"hiphop\": 4,\n",
    "    \"jazz\": 5,\n",
    "    \"metal\": 6,\n",
    "    \"pop\": 7,\n",
    "    \"reggae\": 8,\n",
    "    \"rock\": 9\n",
    "}\n",
    "\n",
    "files = get_all_wav_files('../data/genres/')\n",
    "labels = list(map(lambda x: labelMap[x.split('/')[-1].split('.')[0]], files))\n",
    "X_train, X_test, y_train, y_test = train_test_split(files,labels, random_state=1122)\n",
    "\n",
    "train_dataset = GTZANSegmentsDataset(X_train, y_train)\n",
    "test_dataset = GTZANSegmentsDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=12, shuffle=False, drop_last=False)"
   ],
   "id": "f54022fb51d17375",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T00:17:31.498357Z",
     "start_time": "2025-04-12T00:16:42.311301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "num_classes = 10\n",
    "feature_dim = 768\n",
    "model_dim = 16\n",
    "num_heads = 2\n",
    "num_layers = 4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AudioTransformerClassifier(feature_dim, model_dim, num_heads, num_layers, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "preds_dict = {}\n",
    "\n",
    "# Example training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        segments = batch[\"segments\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(segments)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            segments = batch[\"segments\"].to(device)  # (batch_size, seq_len, feature_dim)\n",
    "            labels = batch[\"label\"].to(device)       # (batch_size,)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(segments)\n",
    "            # Predicted class indices\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Move predictions & labels back to CPU and store\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    genres_list = [\"blues\", \"classical\", \"country\", \"disco\", \"hiphop\", \n",
    "               \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]\n",
    "    report = classification_report(all_preds, all_labels, target_names=genres_list)\n",
    "    print(report)\n",
    "    \n",
    "    preds_dict[epoch] = (all_preds, all_labels)"
   ],
   "id": "19e388d14c09ba3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop.00055.wav\n",
      "hiphop.00014.wav\n",
      "blues.00054.wav\n",
      "country.00046.wav\n",
      "jazz.00051.wav\n",
      "country.00099.wav\n",
      "rock.00090.wav\n",
      "hiphop.00033.wav\n",
      "country.00012.wav\n",
      "hiphop.00029.wav\n",
      "disco.00058.wav\n",
      "metal.00098.wav\n",
      "torch.Size([12, 999, 768])\n",
      "rock.00084.wav\n",
      "reggae.00011.wav\n",
      "country.00088.wav\n",
      "blues.00045.wav\n",
      "blues.00044.wav\n",
      "metal.00085.wav\n",
      "jazz.00008.wav\n",
      "blues.00038.wav\n",
      "rock.00019.wav\n",
      "disco.00053.wav\n",
      "blues.00025.wav\n",
      "classical.00085.wav\n",
      "torch.Size([12, 999, 768])\n",
      "jazz.00067.wav\n",
      "pop.00012.wav\n",
      "reggae.00057.wav\n",
      "classical.00033.wav\n",
      "rock.00021.wav\n",
      "pop.00042.wav\n",
      "metal.00008.wav\n",
      "rock.00074.wav\n",
      "disco.00033.wav\n",
      "metal.00061.wav\n",
      "metal.00051.wav\n",
      "classical.00068.wav\n",
      "torch.Size([12, 999, 768])\n",
      "classical.00081.wav\n",
      "pop.00047.wav\n",
      "disco.00086.wav\n",
      "reggae.00062.wav\n",
      "blues.00033.wav\n",
      "jazz.00094.wav\n",
      "metal.00078.wav\n",
      "rock.00071.wav\n",
      "rock.00080.wav\n",
      "country.00042.wav\n",
      "disco.00090.wav\n",
      "metal.00047.wav\n",
      "torch.Size([12, 999, 768])\n",
      "blues.00061.wav\n",
      "disco.00055.wav\n",
      "classical.00031.wav\n",
      "hiphop.00055.wav\n",
      "rock.00082.wav\n",
      "country.00080.wav\n",
      "jazz.00040.wav\n",
      "classical.00002.wav\n",
      "blues.00013.wav\n",
      "reggae.00029.wav\n",
      "metal.00034.wav\n",
      "pop.00083.wav\n",
      "torch.Size([12, 999, 768])\n",
      "rock.00070.wav\n",
      "blues.00052.wav\n",
      "metal.00055.wav\n",
      "country.00000.wav\n",
      "reggae.00047.wav\n",
      "blues.00006.wav\n",
      "pop.00018.wav\n",
      "metal.00030.wav\n",
      "classical.00087.wav\n",
      "classical.00099.wav\n",
      "jazz.00057.wav\n",
      "pop.00084.wav\n",
      "torch.Size([12, 999, 768])\n",
      "disco.00005.wav\n",
      "classical.00035.wav\n",
      "blues.00039.wav\n",
      "disco.00076.wav\n",
      "country.00056.wav\n",
      "blues.00021.wav\n",
      "rock.00006.wav\n",
      "country.00016.wav\n",
      "classical.00049.wav\n",
      "metal.00075.wav\n",
      "jazz.00075.wav\n",
      "disco.00013.wav\n",
      "torch.Size([12, 999, 768])\n",
      "reggae.00042.wav\n",
      "rock.00077.wav\n",
      "disco.00060.wav\n",
      "pop.00054.wav\n",
      "pop.00081.wav\n",
      "jazz.00041.wav\n",
      "hiphop.00048.wav\n",
      "disco.00077.wav\n",
      "rock.00007.wav\n",
      "jazz.00061.wav\n",
      "classical.00005.wav\n",
      "disco.00099.wav\n",
      "torch.Size([12, 999, 768])\n",
      "pop.00053.wav\n",
      "reggae.00018.wav\n",
      "pop.00051.wav\n",
      "reggae.00023.wav\n",
      "pop.00080.wav\n",
      "jazz.00054.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awast\\AppData\\Local\\Temp\\ipykernel_25100\\3985503370.py:5: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  waveform, original_sr = librosa.load(audio_path, sr=sr, mono=True, duration=20)\n",
      "C:\\Users\\awast\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "ename": "NoBackendError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLibsndfileError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py:176\u001B[0m, in \u001B[0;36mload\u001B[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001B[0m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 176\u001B[0m     y, sr_native \u001B[38;5;241m=\u001B[39m \u001B[43m__soundfile_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mduration\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m sf\u001B[38;5;241m.\u001B[39mSoundFileRuntimeError \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    179\u001B[0m     \u001B[38;5;66;03m# If soundfile failed, try audioread instead\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py:209\u001B[0m, in \u001B[0;36m__soundfile_load\u001B[1;34m(path, offset, duration, dtype)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;66;03m# Otherwise, create the soundfile object\u001B[39;00m\n\u001B[1;32m--> 209\u001B[0m     context \u001B[38;5;241m=\u001B[39m \u001B[43msf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSoundFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    211\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context \u001B[38;5;28;01mas\u001B[39;00m sf_desc:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\soundfile.py:690\u001B[0m, in \u001B[0;36mSoundFile.__init__\u001B[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001B[0m\n\u001B[0;32m    688\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info \u001B[38;5;241m=\u001B[39m _create_info_struct(file, mode, samplerate, channels,\n\u001B[0;32m    689\u001B[0m                                  \u001B[38;5;28mformat\u001B[39m, subtype, endian)\n\u001B[1;32m--> 690\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode_int\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosefd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    691\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mset\u001B[39m(mode)\u001B[38;5;241m.\u001B[39missuperset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr+\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseekable():\n\u001B[0;32m    692\u001B[0m     \u001B[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\soundfile.py:1265\u001B[0m, in \u001B[0;36mSoundFile._open\u001B[1;34m(self, file, mode_int, closefd)\u001B[0m\n\u001B[0;32m   1264\u001B[0m     err \u001B[38;5;241m=\u001B[39m _snd\u001B[38;5;241m.\u001B[39msf_error(file_ptr)\n\u001B[1;32m-> 1265\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m LibsndfileError(err, prefix\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError opening \u001B[39m\u001B[38;5;132;01m{0!r}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname))\n\u001B[0;32m   1266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode_int \u001B[38;5;241m==\u001B[39m _snd\u001B[38;5;241m.\u001B[39mSFM_WRITE:\n\u001B[0;32m   1267\u001B[0m     \u001B[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001B[39;00m\n\u001B[0;32m   1268\u001B[0m     \u001B[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001B[39;00m\n\u001B[0;32m   1269\u001B[0m     \u001B[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001B[39;00m\n",
      "\u001B[1;31mLibsndfileError\u001B[0m: Error opening '../data/genres/jazz/jazz.00054.wav': Format not recognised.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mNoBackendError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 27\u001B[0m\n\u001B[0;32m     24\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     25\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 27\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[43m    \u001B[49m\u001B[43msegments\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msegments\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    705\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    707\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 708\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    709\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    710\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    711\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    712\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    713\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    714\u001B[0m ):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    762\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    763\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 764\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    765\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    766\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[4], line 85\u001B[0m, in \u001B[0;36mGTZANSegmentsDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     82\u001B[0m filename \u001B[38;5;241m=\u001B[39m row\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28mprint\u001B[39m(filename)\n\u001B[1;32m---> 85\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[43mextract_wav2vec_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwav2vec\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprocessor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;66;03m# Convert to a NumPy array or directly to torch.Tensor\u001B[39;00m\n\u001B[0;32m     88\u001B[0m X_np \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(hidden_states, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "Cell \u001B[1;32mIn[3], line 5\u001B[0m, in \u001B[0;36mextract_wav2vec_features\u001B[1;34m(audio_path, sr, model, proc, device)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mextract_wav2vec_features\u001B[39m(audio_path, sr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16000\u001B[39m, model\u001B[38;5;241m=\u001B[39mwav2vec, proc\u001B[38;5;241m=\u001B[39mprocessor, device\u001B[38;5;241m=\u001B[39mdevice):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;66;03m# 1. Load raw audio\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m     waveform, original_sr \u001B[38;5;241m=\u001B[39m \u001B[43mlibrosa\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmono\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mduration\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;66;03m# 2. Wav2Vec2 expects a batch of waveforms in float32\u001B[39;00m\n\u001B[0;32m      8\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m proc(waveform, sampling_rate\u001B[38;5;241m=\u001B[39msr, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py:184\u001B[0m, in \u001B[0;36mload\u001B[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001B[0m\n\u001B[0;32m    180\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, (\u001B[38;5;28mstr\u001B[39m, pathlib\u001B[38;5;241m.\u001B[39mPurePath)):\n\u001B[0;32m    181\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    182\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPySoundFile failed. Trying audioread instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m\n\u001B[0;32m    183\u001B[0m     )\n\u001B[1;32m--> 184\u001B[0m     y, sr_native \u001B[38;5;241m=\u001B[39m \u001B[43m__audioread_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mduration\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    186\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\decorator.py:232\u001B[0m, in \u001B[0;36mdecorate.<locals>.fun\u001B[1;34m(*args, **kw)\u001B[0m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwsyntax:\n\u001B[0;32m    231\u001B[0m     args, kw \u001B[38;5;241m=\u001B[39m fix(args, kw, sig)\n\u001B[1;32m--> 232\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcaller\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mextras\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\util\\decorators.py:63\u001B[0m, in \u001B[0;36mdeprecated.<locals>.__wrapper\u001B[1;34m(func, *args, **kwargs)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001B[39;00m\n\u001B[0;32m     55\u001B[0m warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{:s}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{:s}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mDeprecated as of librosa version \u001B[39m\u001B[38;5;132;01m{:s}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mIt will be removed in librosa version \u001B[39m\u001B[38;5;132;01m{:s}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     61\u001B[0m     stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,  \u001B[38;5;66;03m# Would be 2, but the decorator adds a level\u001B[39;00m\n\u001B[0;32m     62\u001B[0m )\n\u001B[1;32m---> 63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py:240\u001B[0m, in \u001B[0;36m__audioread_load\u001B[1;34m(path, offset, duration, dtype)\u001B[0m\n\u001B[0;32m    237\u001B[0m     reader \u001B[38;5;241m=\u001B[39m path\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    239\u001B[0m     \u001B[38;5;66;03m# If the input was not an audioread object, try to open it\u001B[39;00m\n\u001B[1;32m--> 240\u001B[0m     reader \u001B[38;5;241m=\u001B[39m \u001B[43maudioread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maudio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m reader \u001B[38;5;28;01mas\u001B[39;00m input_file:\n\u001B[0;32m    243\u001B[0m     sr_native \u001B[38;5;241m=\u001B[39m input_file\u001B[38;5;241m.\u001B[39msamplerate\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\audioread\\__init__.py:132\u001B[0m, in \u001B[0;36maudio_open\u001B[1;34m(path, backends)\u001B[0m\n\u001B[0;32m    129\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;66;03m# All backends failed!\u001B[39;00m\n\u001B[1;32m--> 132\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m NoBackendError()\n",
      "\u001B[1;31mNoBackendError\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "839a7b5dcc7c5a37",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
